{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MV2290/MV2290/blob/main/GoogleMaps_selenium_TO_BE_SHARED_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and guide"
      ],
      "metadata": {
        "id": "sNGhOBgL1YFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script permits to scrape a specific GoogleMaps page without needing of GoogleMaps API.\n",
        "Information obtained from script are general overview, information page and reviews.\n",
        "\n",
        "The script is designed completly online, from the platform to output. For this reason you need to connect your GoogleDrive to Google Colab to deliver data to a Google Sheet saved on your Drive. Organization of output is specified for a given template.\n",
        "\n",
        "Remember, the time of scraping depends from your connection and how many reviews are inside website.\n",
        "\n",
        "For a script overview refer to GMS_overview picture."
      ],
      "metadata": {
        "id": "uh_z1mCru8PN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step-by-step guide:**\n",
        "\n",
        "1.   Run point 1. and authorize Google Colab to connect to your Google Drive. Then run point 2. and point 3. (refer to video 'GMS_FirstSession' loaded on files)\n",
        "\n",
        "2.   At point 4. you have to insert the inputs for scraping. (you can check video 'GMS_InputAndExtraction')\n",
        "*   Google sheet name where your output should be stored\n",
        "*   URL of GoogleMaps page\n",
        "*   How many reviews you want to scrap\n",
        "*   Information and reviews Xpath, please check dedicated step-by-step instruction after this point, or the video related (GMS_Xpath) if you are not sure what are the xpath required\n",
        "\n",
        "3.  Point 5. start the extraction, during extraction will appaer some useful information\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJagRD3BKhV5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1EBGb6juBx8"
      },
      "source": [
        "**Instruction to retrive xpath for reviews and information:**\n",
        "\n",
        "\n",
        "1.   Go to GoogleMaps page that you need to scrap\n",
        "2.   Press F12 key (command+option+U on Mac) to be able to see the html code\n",
        "3.   Activate inspector key with ctrl+shift+C (command+option+I on Mac)\n",
        "4.   Click on Reviews or Information button and html code will be evidenced on right side\n",
        "5.   Select html code begining with \"button\" and right click on it\n",
        "6.   On menu list go to Copy and then select \"Copy XPath\"\n",
        "7.   Now you can parse the xpath in your code here above\n",
        "\n",
        "ps Most of the time, xpath is the same, so you don't need to replace it every time you run the code, only when you get the error message from the last code indicating \"Xpath not found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGf4yUvVBKAG"
      },
      "source": [
        "# 1. Authorize and link your Google Drive - *Run once*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4PbgN5pSBH"
      },
      "source": [
        "When you run the first code, automatic pop-up window will appear to authorize the connection between Google Colab and Google Drive. After that, you can run the other code, if you are doing from Section, will be automatic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFp6khULaBbE"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pTbMDXnac8X"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLmasCDhBb51"
      },
      "source": [
        "# 2. Selenium setup - *Run only once*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHWO90MPaDSU"
      },
      "source": [
        "Set up for running selenium in Google Colab\n",
        "You don't need to run this code if you do it in Jupyter notebook, or other local Python setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpDrNkIRYpJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e95776e-275b-4ef2-9f04-85d6b4ba44dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [Waiting for headers] \u001b[0m\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r                                                                                                    \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,304 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,578 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,547 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,308 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,036 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [28.5 kB]\n",
            "Fetched 7,161 kB in 2s (4,641 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 26.4 MB of archives.\n",
            "After this operation, 116 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.11 [78.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.11 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.58+22.04.1 [23.8 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.3 [2,908 B]\n",
            "Fetched 26.4 MB in 1s (41.9 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 120903 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.11) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121111 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.58+22.04.1_amd64.deb ...\n",
            "Unpacking snapd (2.58+22.04.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.11) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.58+22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 121344 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.3_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.11) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.3/448.3 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.16.0 trio-0.23.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import re\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "# set options to be headless, ..\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7WxrhFnCePa"
      },
      "source": [
        "# 3. Extraction information code core - *Run only once*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "CDXnlmtnNmCo"
      },
      "outputs": [],
      "source": [
        "class MapScraper:\n",
        "    def __init__(self, url, file_name):\n",
        "        self.driver = None\n",
        "        self.name = file_name\n",
        "        self.driver = driver\n",
        "        self.unique = []\n",
        "        self.url = url\n",
        "\n",
        "    def search_location(self):\n",
        "        self.driver.get(self.url)\n",
        "        try:\n",
        "          search_accept = self.driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button/span')\n",
        "          search_accept.click()\n",
        "        except: pass\n",
        "        time.sleep(2)\n",
        "\n",
        "        Utils.get_business_info()\n",
        "\n",
        "    def get_business_info(self):\n",
        "        try:\n",
        "            time.sleep(2)\n",
        "            start_time = time.time()\n",
        "            source = self.driver.page_source\n",
        "            soup = BeautifulSoup(source, 'html.parser')\n",
        "            name = Utils.parse_name(soup)\n",
        "            url = Utils.link(self.url)\n",
        "\n",
        "            if name not in self.unique and name:\n",
        "                self.unique.append(name)\n",
        "                url = Utils.link(self.url)\n",
        "                rating, reviews = Utils.parse_rating_and_reviews(soup)\n",
        "                address = Utils.parse_address(soup)\n",
        "                category = Utils.parse_category(soup)\n",
        "                website, phone = Utils.parse_contact_info(soup)\n",
        "                detail = Utils.get_detail(soup)\n",
        "                hours = Utils.parse_time(soup)\n",
        "\n",
        "                record = [\n",
        "                    name,\n",
        "                    address,\n",
        "                    phone,\n",
        "                    website,\n",
        "                    rating,\n",
        "                    reviews,\n",
        "                    hours,\n",
        "                    category,\n",
        "                    detail\n",
        "                ]\n",
        "                time.sleep(2)\n",
        "                print(f'Scraped informations, excel first page: {record}')\n",
        "                info = Utils.get_information()\n",
        "                print(f'Information scrapped: {info}')\n",
        "                time.sleep(2)\n",
        "                reviews = Utils.get_reviews()\n",
        "                print(f'Number of reviews scrapped: {reviews}')\n",
        "                time.sleep(2)\n",
        "                end_time = time.time()\n",
        "                print(f'Scraping time: {round(end_time - start_time)} or {round((end_time - start_time)/60,2)} minutes')\n",
        "\n",
        "                self.driver.quit()\n",
        "\n",
        "        except Exception as err:\n",
        "            print(f'get_business_info: {err}')\n",
        "\n",
        "    # get information page\n",
        "    def get_information(self):\n",
        "        try:\n",
        "            click_info = self.driver.find_element(By.XPATH,info_xpath)\n",
        "            click_info.click()\n",
        "            time.sleep(2)\n",
        "            source = self.driver.page_source\n",
        "            soup = BeautifulSoup(source, 'html.parser')\n",
        "            ws = gc.open(self.name).get_worksheet(1)\n",
        "            info = soup.find(\"div\",{\"class\":\"m6QErb DxyBCb kA9KIf dS8AEf\"})\n",
        "            c = 2\n",
        "            for i in info.find_all(\"div\"):\n",
        "                try:\n",
        "                    cat = i.find('h2').text\n",
        "                    ws.update('B'+str(c),cat)\n",
        "                    for span in i.find_all('span'):\n",
        "                        ws.update('B'+str(c),cat)\n",
        "                        ws.update('C'+str(c),span.text)\n",
        "                        c += 1\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "              back_click = self.driver.find_element(By.XPATH,'//*[@id=\"omnibox-singlebox\"]/div/div[1]/button')\n",
        "              back_click.click()\n",
        "            except:\n",
        "              pass\n",
        "        except Exception as err:\n",
        "            print(f'get_information: {err}')\n",
        "\n",
        "        return \"done\"\n",
        "\n",
        "    def parse_name(self, content):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        activity_name = \"Not found\"\n",
        "        try:\n",
        "            activity_name = content.find('h1', {\"class\": \"DUwDvf\"}).text\n",
        "        except Exception as err:\n",
        "            print(\"parse_name\", err)\n",
        "        ws.update('C2',activity_name)\n",
        "\n",
        "        return activity_name\n",
        "\n",
        "    def parse_address(self, content):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        address = \"Not found\"\n",
        "        try:\n",
        "            address_block = content.find_all('div', {\"class\": \"RcCsl fVHpi w4vB1d NOE9ve M0S7ae AG25L\"})\n",
        "            address = address_block[0].text\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"parse_address\", err)\n",
        "        ws.update('C5',address)\n",
        "        return address\n",
        "\n",
        "    def parse_rating_and_reviews(self,content):\n",
        "        rating = None\n",
        "        reviews = None\n",
        "        try:\n",
        "            rating_area = content.find('div', {\"class\": \"F7nice\"}).text.split(\"(\")\n",
        "            if len(rating_area) > 1:\n",
        "                rating = rating_area[0].strip()\n",
        "                reviews = rating_area[1].split(\")\")[0].strip()\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"parse_rating_and_reviews\", err)\n",
        "\n",
        "        return rating, reviews\n",
        "\n",
        "    def parse_contact_info(self, content):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        website = \"Not found\"\n",
        "        phone = \"Not found\"\n",
        "        try:\n",
        "            pattern = re.compile(r'[a-zA-Z0-9]+\\.+[a-zA-Z]+')\n",
        "            address_block = content.find_all('div', {\"class\": \"RcCsl fVHpi w4vB1d NOE9ve M0S7ae AG25L\"})\n",
        "\n",
        "            for container in address_block:\n",
        "\n",
        "                if pattern.search(container.text):\n",
        "                    website = container.find(\"a\").get('href')\n",
        "\n",
        "                for i in range(1,1000):\n",
        "                    prefix = \"+\"+str(i)\n",
        "\n",
        "                    if prefix in container.text and container.text[0] == \"+\":\n",
        "                        phone = container.text\n",
        "                        break\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"parse_contact_info\", err)\n",
        "\n",
        "        ws.update('C6',phone)\n",
        "        ws.update('C7',website)\n",
        "        return website, phone\n",
        "\n",
        "    def parse_category(self, content):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        category = \"Not found\"\n",
        "        try:\n",
        "            category = content.find('button', {\"class\": \"DkEaL\"}).text\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"parse_category\", err)\n",
        "        ws.update('C4',category)\n",
        "\n",
        "        return category\n",
        "\n",
        "    def parse_time(self, content):\n",
        "        count = 0\n",
        "\n",
        "        #why this loop? Some times happen that format of hours is different, so you need to insert the new XPATH, could be updated\n",
        "        xpath = [ '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[9]/div[4]/div[1]/div[2]/div/span[2]',\n",
        "            '/html/body/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[7]/div[4]/div[1]/div[2]/div/span[2]',\n",
        "        ]\n",
        "\n",
        "        for x in xpath:\n",
        "         try:\n",
        "            open_hours_click = self.driver.find_element(By.XPATH, x)\n",
        "            open_hours_click.click()\n",
        "            count += 1\n",
        "            break\n",
        "         except:\n",
        "            continue\n",
        "\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        days = []\n",
        "        h = []\n",
        "        timing = []\n",
        "        try:\n",
        "            hours = content.find(\"table\",{\"class\":\"eK4R0e fontBodyMedium\"})\n",
        "            for i in hours.find_all(\"div\"):\n",
        "                try:\n",
        "                    if i.text != \"\":\n",
        "                        days.append(i.text + \": \")\n",
        "                except:\n",
        "                    continue\n",
        "            for i in hours.find_all(\"ul\"):\n",
        "                try:\n",
        "                    if i.text != \"\":\n",
        "                        h.append(i.text + \" \")\n",
        "                except:\n",
        "                    continue\n",
        "            for t in range(len(days)):\n",
        "                try:\n",
        "                    timing.append(days[t])\n",
        "                    timing.append(h[t])\n",
        "                except: break\n",
        "            hours = \"\".join(timing)\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"parse_time\", err)\n",
        "        ws.update('C8',hours)\n",
        "\n",
        "        return hours\n",
        "\n",
        "    def get_detail(self, content):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        detail = \"Not found\"\n",
        "        try:\n",
        "            detail = content.find('div', {\"class\": \"PYvSYb\"}).text\n",
        "\n",
        "        except Exception as err:\n",
        "            print(\"get_detail\", err)\n",
        "        ws.update('C9',detail)\n",
        "\n",
        "        return detail\n",
        "\n",
        "    def link(self,url):\n",
        "        ws = gc.open(self.name).sheet1\n",
        "        ws.update('C3',url)\n",
        "\n",
        "        return url\n",
        "\n",
        "    # get reviews\n",
        "    def get_reviews(self):\n",
        "        click_info = self.driver.find_element(By.XPATH,reviews_xpath)\n",
        "        click_info.click()\n",
        "        time.sleep(3)\n",
        "        reviews = []\n",
        "        try:\n",
        "\n",
        "          # Wait for the reviews to load\n",
        "          wait = WebDriverWait(self.driver, 20)  # Increased the waiting time\n",
        "\n",
        "          # Scroll down to load more reviews\n",
        "          body = self.driver.find_element(By.XPATH, \"//div[contains(@class, 'm6QErb') and contains(@class, 'DxyBCb') and contains(@class, 'kA9KIf') and contains(@class, 'dS8AEf')]\")\n",
        "\n",
        "          num_reviews = len(self.driver.find_elements(By.CLASS_NAME, 'wiI7pd'))\n",
        "\n",
        "          while True:\n",
        "              body.send_keys(Keys.END)\n",
        "              time.sleep(3)  # Adjust the delay based on your internet speed and page loading time\n",
        "              new_num_reviews = len(self.driver.find_elements(By.CLASS_NAME, 'wiI7pd'))\n",
        "              if new_num_reviews == num_reviews:\n",
        "                  # Scroll to the top to ensure all reviews are loaded\n",
        "                  body.send_keys(Keys.HOME)\n",
        "                  time.sleep(3)\n",
        "                  break\n",
        "              num_reviews = new_num_reviews\n",
        "\n",
        "          # Wait for the reviews to load completely\n",
        "          wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'wiI7pd')))\n",
        "\n",
        "          # Let all long reviews visible\n",
        "          complete_review = self.driver.find_elements(By.CLASS_NAME,'w8nwRe kyuRq')\n",
        "          for cr in complete_review:\n",
        "            cr.click()\n",
        "\n",
        "          # Extract the text of each review\n",
        "          ws = gc.open(self.name).get_worksheet(2)\n",
        "\n",
        "          review_name = driver.find_elements(By.CLASS_NAME, 'd4r55')\n",
        "          review_time = driver.find_elements(By.CLASS_NAME, 'rsqaWe')\n",
        "          review_stars = driver.find_elements(By.CLASS_NAME, 'kvMYJc')\n",
        "          review_text = driver.find_elements(By.CLASS_NAME, 'wiI7pd')\n",
        "\n",
        "          if len(review_text) > reviews_limit:\n",
        "            review_name = review_name[:reviews_limit]\n",
        "            review_time = review_time[:reviews_limit]\n",
        "            review_stars = review_stars[:reviews_limit]\n",
        "            review_text = review_text[:reviews_limit]\n",
        "            len_review = reviews_limit\n",
        "          else:\n",
        "            len_review = len(review_text)\n",
        "\n",
        "          reviews1 = [element.text for element in review_name]\n",
        "          reviews2 = [element.text for element in review_time]\n",
        "          reviews3 = [element.get_attribute(\"aria-label\") for element in review_stars]\n",
        "          reviews4 = [element.text for element in review_text]\n",
        "          cell = 2\n",
        "\n",
        "          for i in range(len(reviews1)):\n",
        "\n",
        "            ws.update(\"B\"+str(cell),i)\n",
        "            ws.update(\"C\"+str(cell),reviews3[i])\n",
        "            ws.update(\"D\"+str(cell),reviews1[i])\n",
        "            ws.update(\"E\"+str(cell),reviews2[i])\n",
        "            ws.update(\"F\"+str(cell),reviews4[i])\n",
        "            cell +=1\n",
        "\n",
        "          reviews = [reviews1,reviews2,reviews3,reviews4]\n",
        "\n",
        "        except Exception as err:\n",
        "          print('get_reviews',err)\n",
        "\n",
        "        return print(f\"Number of reviews scraped: {len_review}\", len(review_name), len(review_time), len(review_stars), len(review_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSkG79oZCsOd"
      },
      "source": [
        "# 4. Start extraction - *Insert Google sheet name, URL and xpath*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX1dIW5Rrm2C"
      },
      "source": [
        "Extract information to Google Sheet:\n",
        "\n",
        "1.   google_sheet_name = insert the name of file (should be always inside '')\n",
        "2.   url = copy paste url to scrape (should be always inside '')\n",
        "\n",
        "**This is the unique cell that you have to modify with GS name and url:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYkPOuneNoVe"
      },
      "outputs": [],
      "source": [
        "#google sheet name inside quotes\n",
        "google_sheet_name = 'Google Maps Data V01'\n",
        "#url you want to scrape\n",
        "url = 'https://maps.app.goo.gl/3q4QM3QW7qJXoZyc9'\n",
        "#limit of reviews you want to scrape\n",
        "reviews_limit = 100\n",
        "#you have to insert the xpath of reviews, please see how from instructions or video (GM_Scrape_RetriveXpath)\n",
        "reviews_xpath = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]'\n",
        "#you have to insert the xpath of information table, please see how from instructions or video (GM_Scrape_RetriveXpath)\n",
        "info_xpath = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[3]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asJxJ5HT_HTc",
        "outputId": "4e7f8d0d-0170-4a97-8990-79da3102ec07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_3LQHiXtk9"
      },
      "source": [
        "# 5. Start extraction - *Run to start extraction*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmYGPrtoW_Ae"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhqu90YiL0Lh"
      },
      "source": [
        "The script returns also any error or information not found in the page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYBaH3vsFZe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafc0dd2-cf99-4f5a-8a7f-1069671411a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_detail 'NoneType' object has no attribute 'text'\n",
            "Scraped informations, excel first page: ['FerreroLegno S.p.A.', 'SS28, 26, 12060 Magliano Alpi CN, Italy', '+39 0174 622411', 'http://www.ferrerolegno.com/', '3.9', '104', 'Tuesday: 8\\u202fAM–5:30\\u202fPM Wednesday: 8\\u202fAM–12\\u202fPM1:30–5:30\\u202fPM Thursday: 8\\u202fAM–12\\u202fPM1:30–5:30\\u202fPM Friday: 8\\u202fAM–12\\u202fPM1:30–5:30\\u202fPM Saturday: Closed Sunday: Closed Monday: 8\\u202fAM–12\\u202fPM1:30–5:30\\u202fPM ', 'Door supplier', 'Not found']\n",
            "Information scrapped: done\n",
            "get_reviews {'code': 429, 'message': \"Quota exceeded for quota metric 'Write requests' and limit 'Write requests per minute per user' of service 'sheets.googleapis.com' for consumer 'project_number:522309567947'.\", 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'RATE_LIMIT_EXCEEDED', 'domain': 'googleapis.com', 'metadata': {'quota_limit': 'WriteRequestsPerMinutePerUser', 'service': 'sheets.googleapis.com', 'quota_metric': 'sheets.googleapis.com/write_requests', 'consumer': 'projects/522309567947', 'quota_location': 'global', 'quota_limit_value': '300'}}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Request a higher quota limit.', 'url': 'https://cloud.google.com/docs/quota#requesting_higher_quota'}]}]}\n",
            "Number of reviews scraped: 79 100 100 100 79\n",
            "Number of reviews scrapped: None\n",
            "Scraping time: 107 or 1.79 minutes\n"
          ]
        }
      ],
      "source": [
        "Utils = MapScraper(url, google_sheet_name)\n",
        "location = Utils.search_location()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sNGhOBgL1YFe",
        "VGf4yUvVBKAG",
        "tLmasCDhBb51",
        "BSkG79oZCsOd",
        "TH_3LQHiXtk9"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}